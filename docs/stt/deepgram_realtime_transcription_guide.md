# Deepgram SDK - Transcri√ß√£o de Fala em Tempo Real
## Guia Completo para Agentes de Voz com Detec√ß√£o de T√©rmino de Fala

Documenta√ß√£o oficial baseada em: https://developers.deepgram.com

---

## √çndice
1. [Vis√£o Geral](#vis√£o-geral)
2. [Conceitos Principais](#conceitos-principais)
3. [Instala√ß√£o](#instala√ß√£o)
4. [Listen V1 - Transcri√ß√£o em Tempo Real](#listen-v1---transcri√ß√£o-em-tempo-real)
5. [Listen V2 - Voice Agent API](#listen-v2---voice-agent-api)
6. [Detec√ß√£o de T√©rmino de Fala](#detec√ß√£o-de-t√©rmino-de-fala)
7. [Exemplos Completos](#exemplos-completos)
8. [Par√¢metros Importantes](#par√¢metros-importantes)
9. [Eventos WebSocket](#eventos-websocket)

---

## Vis√£o Geral

O Deepgram SDK Python fornece duas APIs principais para transcri√ß√£o de fala em tempo real:

### **Listen V1** (`/v1/listen`)
- API de transcri√ß√£o WebSocket em tempo real
- Ideal para transcri√ß√£o cont√≠nua de √°udio
- Controle granular sobre par√¢metros de transcri√ß√£o

### **Listen V2** (`/v2/listen`)
- Nova API otimizada para Voice Agents
- Detec√ß√£o autom√°tica de turnos de conversa√ß√£o
- Melhor para aplica√ß√µes de assistente de voz conversacional

---

## Conceitos Principais

### **VAD (Voice Activity Detection)**
Detecta quando o usu√°rio est√° ou n√£o falando. √â fundamental para:
- Iniciar a transcri√ß√£o apenas quando h√° fala
- Detectar pausas na fala
- Identificar quando o usu√°rio terminou de falar

### **Endpointing**
Configura o tempo de sil√™ncio necess√°rio para considerar que o usu√°rio terminou uma utterance (enunciado).

### **UtteranceEnd**
Evento que confirma o final de uma utterance completa, usado em conjunto com VAD e endpointing.

### **speech_final**
Flag que indica quando uma transcri√ß√£o est√° finalizada devido √† detec√ß√£o de uma pausa.

---

## Instala√ß√£o

```bash
pip install deepgram-sdk
```

Ou com Poetry/uv:
```bash
poetry add deepgram-sdk
# ou
uv add deepgram-sdk
```

---

## Listen V1 - Transcri√ß√£o em Tempo Real

### Conex√£o B√°sica (S√≠ncrona)

```python
from deepgram import DeepgramClient, LiveOptions
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV1SocketClientResponse

# Inicializar cliente
API_KEY = "sua_chave_api_aqui"
client = DeepgramClient(API_KEY)

# Conectar ao WebSocket
with client.listen.v1.connect(
    model="nova-3",
    language="en-US",  # ou "pt-BR" para portugu√™s
    smart_format=True,
    encoding="linear16",
    channels=1,
    sample_rate=16000,
) as connection:
    def on_message(message: ListenV1SocketClientResponse) -> None:
        if hasattr(message, 'channel') and message.channel:
            transcript = message.channel.alternatives[0].transcript
            if len(transcript) > 0:
                print(f"Transcri√ß√£o: {transcript}")

    # Registrar event handlers
    connection.on(EventType.OPEN, lambda _: print("Conex√£o aberta"))
    connection.on(EventType.MESSAGE, on_message)
    connection.on(EventType.CLOSE, lambda _: print("Conex√£o fechada"))
    connection.on(EventType.ERROR, lambda error: print(f"Erro: {error}"))

    # Iniciar listening
    connection.start_listening()

    # Enviar √°udio
    from deepgram.extensions.types.sockets import ListenV1MediaMessage
    with open("audio.wav", "rb") as audio_file:
        while data := audio_file.read(1024):
            connection.send_media(ListenV1MediaMessage(data))
```

### Conex√£o B√°sica (Ass√≠ncrona)

```python
import asyncio
from deepgram import AsyncDeepgramClient
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV1SocketClientResponse

API_KEY = "sua_chave_api_aqui"
client = AsyncDeepgramClient(API_KEY)

async def transcribe_audio():
    async with client.listen.v1.connect(
        model="nova-3",
        language="pt-BR",
        smart_format=True,
        encoding="linear16",
        sample_rate=16000,
    ) as connection:
        def on_message(message: ListenV1SocketClientResponse) -> None:
            if hasattr(message, 'channel') and message.channel:
                transcript = message.channel.alternatives[0].transcript
                if len(transcript) > 0:
                    print(f"Transcri√ß√£o: {transcript}")

        connection.on(EventType.OPEN, lambda _: print("Conex√£o aberta"))
        connection.on(EventType.MESSAGE, on_message)
        connection.on(EventType.CLOSE, lambda _: print("Conex√£o fechada"))

        await connection.start_listening()

        # Enviar √°udio em chunks
        from deepgram.extensions.types.sockets import ListenV1MediaMessage
        with open("audio.wav", "rb") as audio_file:
            while data := audio_file.read(1024):
                await connection.send_media(ListenV1MediaMessage(data))

        # Aguardar processamento
        await asyncio.sleep(2)

asyncio.run(transcribe_audio())
```

---

## Listen V2 - Voice Agent API

**Listen V2** √© a API recomendada para Voice Agents, pois inclui:
- Detec√ß√£o autom√°tica de turnos (End-of-Turn Detection)
- Melhor lat√™ncia
- Configura√ß√µes otimizadas para conversa√ß√£o

### Conex√£o V2 (S√≠ncrona)

```python
from deepgram import DeepgramClient
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV2SocketClientResponse

API_KEY = "sua_chave_api_aqui"
client = DeepgramClient(API_KEY)

with client.listen.v2.connect(
    model="flux-general-en",  # ou "pt-BR" para portugu√™s
    encoding="linear16",
    sample_rate="16000",
    # Par√¢metros de detec√ß√£o de turno
    eager_eot_threshold="0.5",  # Threshold para eager end-of-turn
    eot_threshold="0.7",  # Threshold para end-of-turn
    eot_timeout_ms="1000",  # Timeout em milliseconds
) as connection:
    def on_message(message: ListenV2SocketClientResponse) -> None:
        msg_type = getattr(message, "type", "Unknown")

        if msg_type == "Welcome":
            print("Conectado ao Deepgram Voice Agent API")

        elif msg_type == "ConversationText":
            # Texto da conversa recebido
            if hasattr(message, 'content'):
                print(f"Usu√°rio disse: {message.content}")

        elif msg_type == "UserStartedSpeaking":
            print("üó£Ô∏è Usu√°rio come√ßou a falar")

        elif msg_type == "AgentThinking":
            print("ü§î Agente est√° pensando...")

        elif msg_type == "AgentStartedSpeaking":
            print("üîä Agente come√ßou a falar")

        elif msg_type == "AgentAudioDone":
            print("‚úÖ √Åudio do agente finalizado")

    # Registrar event handlers
    connection.on(EventType.OPEN, lambda _: print("Conex√£o aberta"))
    connection.on(EventType.MESSAGE, on_message)
    connection.on(EventType.CLOSE, lambda _: print("Conex√£o fechada"))
    connection.on(EventType.ERROR, lambda error: print(f"Erro: {error}"))

    connection.start_listening()

    # Enviar √°udio
    from deepgram.extensions.types.sockets import ListenV2MediaMessage
    with open("microphone_audio.wav", "rb") as audio_file:
        while data := audio_file.read(1024):
            connection.send_media(ListenV2MediaMessage(data=data))
```

### Conex√£o V2 (Ass√≠ncrona)

```python
import asyncio
from deepgram import AsyncDeepgramClient
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV2SocketClientResponse

API_KEY = "sua_chave_api_aqui"
client = AsyncDeepgramClient(API_KEY)

async def voice_agent():
    async with client.listen.v2.connect(
        model="flux-general-en",
        encoding="linear16",
        sample_rate="16000",
    ) as connection:
        def on_message(message: ListenV2SocketClientResponse) -> None:
            msg_type = getattr(message, "type", "Unknown")

            if msg_type == "ConversationText":
                if hasattr(message, 'content'):
                    print(f"Transcri√ß√£o: {message.content}")

        connection.on(EventType.OPEN, lambda _: print("Conex√£o aberta"))
        connection.on(EventType.MESSAGE, on_message)

        await connection.start_listening()

        # Enviar √°udio do microfone
        from deepgram.extensions.types.sockets import ListenV2MediaMessage
        # Aqui voc√™ usaria uma biblioteca para capturar √°udio do microfone
        # Exemplo com pyaudio:
        # import pyaudio
        # audio = pyaudio.PyAudio()
        # stream = audio.open(format=pyaudio.paInt16, channels=1,
        #                     rate=16000, input=True, frames_per_buffer=1024)
        #
        # while True:
        #     data = stream.read(1024)
        #     await connection.send_media(ListenV2MediaMessage(data=data))

asyncio.run(voice_agent())
```

---

## Detec√ß√£o de T√©rmino de Fala

### M√©todo 1: Endpointing (Listen V1)

O `endpointing` define quanto tempo de sil√™ncio (em ms) √© necess√°rio para considerar que o usu√°rio terminou.

```python
with client.listen.v1.connect(
    model="nova-3",
    language="pt-BR",
    endpointing=500,  # 500ms de sil√™ncio para finalizar
    interim_results=True,  # Necess√°rio para resultados parciais
    smart_format=True,
) as connection:
    def on_message(message: ListenV1SocketClientResponse) -> None:
        if hasattr(message, 'channel') and message.channel:
            transcript = message.channel.alternatives[0].transcript

            # Verificar se √© resultado final
            is_final = getattr(message, 'is_final', False)
            speech_final = getattr(message, 'speech_final', False)

            if len(transcript) > 0:
                print(f"[{'FINAL' if speech_final else 'PARCIAL'}] {transcript}")

                if speech_final:
                    print("‚úÖ Usu√°rio terminou de falar!")
                    # Aqui voc√™ pode processar o comando

    connection.on(EventType.MESSAGE, on_message)
    connection.start_listening()
```

### M√©todo 2: UtteranceEnd Events (Listen V1)

Mais preciso que endpointing, o `utterance_end_ms` define quanto tempo aguardar ap√≥s a √∫ltima palavra antes de emitir evento `UtteranceEnd`.

```python
with client.listen.v1.connect(
    model="nova-3",
    language="pt-BR",
    # Configura√ß√µes para UtteranceEnd
    interim_results=True,
    utterance_end_ms="1000",  # 1 segundo ap√≥s a √∫ltima palavra
    vad_events=True,  # Necess√°rio para eventos VAD
    endpointing=300,
) as connection:
    def on_message(message: ListenV1SocketClientResponse) -> None:
        msg_type = getattr(message, "type", "Unknown")

        if msg_type == "SpeechStarted":
            print("üó£Ô∏è Usu√°rio come√ßou a falar")

        elif msg_type == "UtteranceEnd":
            # UtteranceEnd √© emitido quando a utterance est√° realmente finalizada
            print("‚úÖ UTTERANCE FINALIZADA - Usu√°rio terminou de falar!")
            # Processar comando completo aqui

        elif hasattr(message, 'channel') and message.channel:
            transcript = message.channel.alternatives[0].transcript
            if len(transcript) > 0:
                speech_final = getattr(message, 'speech_final', False)
                print(f"[{'F' if speech_final else 'P'}] {transcript}")

    connection.on(EventType.MESSAGE, on_message)
    connection.start_listening()
```

### M√©todo 3: Voice Agent API EOT Detection (Listen V2)

**Recomendado para Voice Agents** - Detec√ß√£o autom√°tica de turnos conversacionais.

```python
with client.listen.v2.connect(
    model="flux-general-en",
    encoding="linear16",
    sample_rate="16000",
    # Controle de detec√ß√£o de fim de turno
    eager_eot_threshold="0.5",  # 0.0 a 1.0 - mais agressivo
    eot_threshold="0.7",  # 0.0 a 1.0 - mais conservador
    eot_timeout_ms="1000",  # Timeout m√°ximo em ms
) as connection:
    def on_message(message: ListenV2SocketClientResponse) -> None:
        msg_type = getattr(message, "type", "Unknown")

        if msg_type == "UserStartedSpeaking":
            print("üó£Ô∏è Usu√°rio come√ßou a falar")

        elif msg_type == "ConversationText":
            if hasattr(message, 'content'):
                print(f"üìù Transcri√ß√£o: {message.content}")
                # Este evento √© emitido quando h√° texto da conversa

        elif msg_type == "AgentThinking":
            print("ü§î Usu√°rio parou de falar, agente processando...")

        elif msg_type == "AgentStartedSpeaking":
            print("üîä Agente respondendo...")

        # Nota: Listen V2 gerencia automaticamente a detec√ß√£o de fim de turno

    connection.on(EventType.MESSAGE, on_message)
    connection.start_listening()
```

---

## Exemplos Completos

### Exemplo 1: Assistente de Voz com Microfone (V1 + Endpointing)

```python
import pyaudio
from deepgram import DeepgramClient, LiveOptions
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV1SocketClientResponse, ListenV1MediaMessage

API_KEY = "sua_chave_api_aqui"

class VoiceAssistant:
    def __init__(self):
        self.client = DeepgramClient(API_KEY)
        self.connection = None
        self.is_listening = False
        self.current_transcript = ""

    def on_message(self, message: ListenV1SocketClientResponse) -> None:
        if hasattr(message, 'channel') and message.channel:
            transcript = message.channel.alternatives[0].transcript
            is_final = getattr(message, 'is_final', False)
            speech_final = getattr(message, 'speech_final', False)

            if len(transcript) > 0:
                if not is_final:
                    # Resultado parcial
                    print(f"\rüé§ {transcript}", end="")
                else:
                    # Resultado final
                    print(f"\r‚úÖ {transcript}")
                    self.current_transcript = transcript

                if speech_final:
                    print("\nüéØ Usu√°rio terminou! Processando comando...")
                    self.process_command(self.current_transcript)
                    self.current_transcript = ""

    def process_command(self, text: str):
        """Processar o comando do usu√°rio"""
        # Aqui voc√™ integra com seu agente
        print(f"üìù Comando recebido: {text}")
        # TODO: Enviar para agente, processar, etc.

    def start(self):
        """Iniciar escuta do microfone"""
        # Configurar √°udio
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )

        # Conectar ao Deepgram
        self.connection = self.client.listen.v1.connect(
            model="nova-3",
            language="pt-BR",
            endpointing=600,  # 600ms de sil√™ncio
            interim_results=True,
            smart_format=True,
            punctuate=True,
            encoding="linear16",
            channels=1,
            sample_rate=16000,
        )

        self.connection.on(EventType.MESSAGE, self.on_message)
        self.connection.on(EventType.OPEN, lambda _: print("üé§ Escutando..."))
        self.connection.on(EventType.ERROR, lambda e: print(f"Erro: {e}"))

        self.connection.start_listening()
        self.is_listening = True

        print("üé§ Assistente de voz ativo! Pressione Ctrl+C para parar.")

        try:
            while self.is_listening:
                # Ler √°udio do microfone e enviar para Deepgram
                data = self.stream.read(1024, exception_on_overflow=False)
                self.connection.send_media(ListenV1MediaMessage(data))
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è Parando assistente...")
            self.stop()

    def stop(self):
        """Parar escuta"""
        self.is_listening = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        if self.audio:
            self.audio.terminate()
        if self.connection:
            self.connection.finish()

if __name__ == "__main__":
    assistant = VoiceAssistant()
    assistant.start()
```

### Exemplo 2: Voice Agent com V2 (Ass√≠ncrono)

```python
import asyncio
import pyaudio
from deepgram import AsyncDeepgramClient
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import ListenV2SocketClientResponse, ListenV2MediaMessage

API_KEY = "sua_chave_api_aqui"

class AsyncVoiceAgent:
    def __init__(self):
        self.client = AsyncDeepgramClient(API_KEY)
        self.connection = None
        self.is_running = False

    def on_message(self, message: ListenV2SocketClientResponse) -> None:
        msg_type = getattr(message, "type", "Unknown")

        if msg_type == "Welcome":
            print("‚úÖ Conectado ao Deepgram Voice Agent API")

        elif msg_type == "UserStartedSpeaking":
            print("üó£Ô∏è ")

        elif msg_type == "ConversationText":
            if hasattr(message, 'content'):
                print(f"üìù Voc√™ disse: {message.content}")
                self.handle_user_input(message.content)

        elif msg_type == "AgentThinking":
            print("ü§î Processando...")

        elif msg_type == "AgentStartedSpeaking":
            print("üîä Agente: ", end="")

        elif msg_type == "AgentAudioDone":
            print()  # Nova linha ap√≥s resposta

    def handle_user_input(self, text: str):
        """Processar entrada do usu√°rio"""
        # Aqui voc√™ integra com seu agente Claude/LLM
        print(f"üí≠ Processando: {text}")
        # TODO: Implementar l√≥gica do agente

    async def stream_microphone(self):
        """Capturar e streaming √°udio do microfone"""
        audio = pyaudio.PyAudio()
        stream = audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )

        print("üé§ Voice Agent ativo! Fale algo...")

        try:
            while self.is_running:
                data = stream.read(1024, exception_on_overflow=False)
                await self.connection.send_media(ListenV2MediaMessage(data=data))
                await asyncio.sleep(0.001)  # Pequeno delay
        finally:
            stream.stop_stream()
            stream.close()
            audio.terminate()

    async def start(self):
        """Iniciar voice agent"""
        self.connection = self.client.listen.v2.connect(
            model="flux-general-en",
            encoding="linear16",
            sample_rate="16000",
            eager_eot_threshold="0.5",
            eot_threshold="0.7",
            eot_timeout_ms="1000",
        )

        self.connection.on(EventType.OPEN, lambda _: print("üîå Conectando..."))
        self.connection.on(EventType.MESSAGE, self.on_message)
        self.connection.on(EventType.CLOSE, lambda _: print("üîå Desconectado"))
        self.connection.on(EventType.ERROR, lambda e: print(f"‚ùå Erro: {e}"))

        await self.connection.start_listening()
        self.is_running = True

        # Iniciar streaming de microfone
        await self.stream_microphone()

    async def stop(self):
        """Parar voice agent"""
        self.is_running = False
        if self.connection:
            await self.connection.send_control(
                ListenV2ControlMessage(type="CloseStream")
            )

if __name__ == "__main__":
    agent = AsyncVoiceAgent()
    try:
        asyncio.run(agent.start())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Parando...")
        asyncio.run(agent.stop())
```

---

## Par√¢metros Importantes

### Par√¢metros de Transcri√ß√£o

| Par√¢metro | Tipo | Descri√ß√£o | Valor Padr√£o |
|-----------|------|-----------|--------------|
| `model` | string | Modelo de IA a usar | `"nova-3"` |
| `language` | string | Idioma (BCP-47) | `"en-US"` |
| `smart_format` | bool | Formata√ß√£o inteligente | `false` |
| `punctuate` | bool | Adicionar pontua√ß√£o | `false` |
| `profanity_filter` | bool | Filtrar profanidade | `false` |
| `diarize` | bool | Reconhecer falantes | `false` |
| `numerals` | bool | Converter n√∫meros | `false` |

### Par√¢metros de √Åudio

| Par√¢metro | Tipo | Descri√ß√£o | Valor Padr√£o |
|-----------|------|-----------|--------------|
| `encoding` | string | Formato de √°udio | Opcional |
| `sample_rate` | int | Taxa de amostragem (Hz) | Opcional |
| `channels` | int | N√∫mero de canais | `1` |

### Par√¢metros de Endpointing (V1)

| Par√¢metro | Tipo | Descri√ß√£o | Valor Padr√£o |
|-----------|------|-----------|--------------|
| `endpointing` | int/bool | Tempo de sil√™ncio (ms) | `10` |
| `interim_results` | bool | Resultados parciais | `false` |
| `utterance_end_ms` | int | Tempo para utterance end | N/A |
| `vad_events` | bool | Eventos VAD | `false` |

### Par√¢metros de EOT Detection (V2)

| Par√¢metro | Tipo | Descri√ß√£o | Valor Padr√£o |
|-----------|------|-----------|--------------|
| `eager_eot_threshold` | float | Threshold eager EOT (0-1) | N/A |
| `eot_threshold` | float | Threshold EOT (0-1) | N/A |
| `eot_timeout_ms` | int | Timeout EOT (ms) | N/A |

---

## Eventos WebSocket

### Listen V1 Events

| Evento | Descri√ß√£o |
|--------|-----------|
| `Metadata` | Informa√ß√µes de metadados da conex√£o |
| `Results` | Resultados de transcri√ß√£o |
| `SpeechStarted` | Usu√°rio come√ßou a falar |
| `UtteranceEnd` | Utterance finalizada |
| `error` | Erro na conex√£o |

### Listen V2 Events

| Evento | Descri√ß√£o |
|--------|-----------|
| `Welcome` | Mensagem de boas-vindas |
| `ConversationText` | Texto da conversa |
| `UserStartedSpeaking` | Usu√°rio come√ßou a falar |
| `AgentThinking` | Agente processando |
| `AgentStartedSpeaking` | Agente come√ßou a falar |
| `AgentAudioDone` | √Åudio do agente finalizado |
| `SettingsApplied` | Configura√ß√µes aplicadas |
| `Warning` | Aviso |
| `Error` | Erro |

---

## Modelos Dispon√≠veis

### Nova Series (Recomendado)
- **nova-3**: Modelo mais avan√ßado, melhor precis√£o
- **nova-2**: Balanceado entre performance e custo

### Flux Series (V2 / Voice Agents)
- **flux-general-en**: Ingl√™s geral para voice agents
- **flux-pt**: Portugu√™s para voice agents

### Idiomas Suportados
- `en-US`: Ingl√™s (EUA)
- `pt-BR`: Portugu√™s (Brasil)
- `es`: Espanhol
- `fr`: Franc√™s
- `de`: Alem√£o
- `it`: Italiano
- E muitos mais...

---

## Melhores Pr√°ticas

### 1. Escolha da API
- **Use Listen V1** para transcri√ß√£o cont√≠nua e controle granular
- **Use Listen V2** para voice agents e conversa√ß√£o natural

### 2. Configura√ß√£o de Endpointing
```python
# Para comandos curtos (assistente pessoal)
endpointing=400-600

# Para conversa√ß√£o natural
endpointing=800-1200

# Para ditado (pausas mais longas)
endpointing=1500-2000
```

### 3. Uso de UtteranceEnd vs speech_final
```python
# speech_final: Mais r√°pido, menos preciso
# Use para: feedback visual em tempo real

# UtteranceEnd: Mais lento, mais preciso
# Use para: processamento de comandos, a√ß√µes definitivas
```

### 4. Tratamento de Erros
```python
def on_message(self, message):
    try:
        # Processar mensagem
        pass
    except Exception as e:
        print(f"Erro processando mensagem: {e}")
        # Reconectar se necess√°rio
```

### 5. Gerenciamento de Conex√£o
```python
# Sempre fechar a conex√£o
try:
    connection.start_listening()
    # ... usar conex√£o
finally:
    connection.finish()
```

---

## Links √öteis

- **Documenta√ß√£o Oficial**: https://developers.deepgram.com
- **GitHub Python SDK**: https://github.com/deepgram/deepgram-python-sdk
- **API Reference**: https://developers.deepgram.com/reference
- **Exemplos**: https://github.com/deepgram/deepgram-python-sdk/tree/main/examples

---

## Conclus√£o

O Deepgram SDK oferece duas abordagens poderosas para transcri√ß√£o em tempo real:

1. **Listen V1** com `endpointing` e `utterance_end_ms` para detec√ß√£o precisa de quando o usu√°rio termina de falar
2. **Listen V2** com detec√ß√£o autom√°tica de turnos conversacionais para voice agents

Para **assistentes de voz**, recomenda-se:
- **V1 + UtteranceEnd** para m√°ximo controle
- **V2** para implementa√ß√£o mais simples e conversa√ß√£o natural

Lembre-se de configurar adequadamente os tempos de sil√™ncio (`endpointing`, `utterance_end_ms`, `eot_timeout_ms`) conforme o uso pretendido.

---

**√öltima atualiza√ß√£o**: 2025-02-15
**Vers√£o do SDK**: 3.x+
**API Listen**: V1 e V2
